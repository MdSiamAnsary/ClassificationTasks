# -*- coding: utf-8 -*-
"""Code[all-mpnet-base-v2].ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dFbfaMTxVW7Fw0dURNVEReiI2sOkvxB_
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install catboost

!pip install datasets

!pip install lightgbm

import sys
import gc

import pandas as pd
from sklearn.model_selection import StratifiedKFold
import numpy as np
from sklearn.metrics import roc_auc_score
import numpy as np
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.feature_extraction.text import TfidfVectorizer

from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

from datasets import Dataset
from tqdm.auto import tqdm
from transformers import PreTrainedTokenizerFast

from sklearn.linear_model import SGDClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import VotingClassifier

dataset = pd.read_csv('dataset.csv')

dataset.head()

df = dataset.drop_duplicates(subset=['text'])
df.reset_index(drop=True, inplace=True)

df.head()

df.drop(['prompt_name', 'source', 'RDizzl3_seven'], axis='columns', inplace=True)

df.head()

df.shape

import sklearn
df = sklearn.utils.shuffle(df)

df.shape

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

!pip install sentence-transformers scikit-learn catboost lightgbm

from sentence_transformers import SentenceTransformer
from sklearn.ensemble import VotingClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import SGDClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report
import numpy as np

from transformers import AutoTokenizer, AutoModel
import torch

texts = df['text'].astype(str).tolist()
labels = df['label'].tolist()

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

bert_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)

X = bert_model.encode(texts, show_progress_bar=True, device=device)



scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, labels, test_size=0.3, random_state=42)

from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier, RandomForestClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import classification_report
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier

xgb = XGBClassifier(
    n_estimators=500, learning_rate=0.05, max_depth=5, subsample=0.8, colsample_bytree=0.8,
    reg_alpha=0.5, reg_lambda=1, use_label_encoder=False, eval_metric='logloss'
)

et = ExtraTreesClassifier(n_estimators=300, max_depth=10, min_samples_split=4, random_state=42)

bag = BaggingClassifier(n_estimators=100, max_samples=0.8, max_features=0.8, random_state=42)

ada = AdaBoostClassifier(n_estimators=200, learning_rate=0.5, random_state=42)

rf = RandomForestClassifier(n_estimators=300, max_depth=10, min_samples_split=4, random_state=42)

cat = CatBoostClassifier(iterations=500, depth=6, learning_rate=0.03, l2_leaf_reg=5, verbose=0)

sgd = SGDClassifier(
    loss='log_loss', penalty='elasticnet', alpha=1e-4, l1_ratio=0.15,
    learning_rate='optimal', early_stopping=True, max_iter=2000, random_state=42
)

lgbm = LGBMClassifier(
    n_estimators=1000, learning_rate=0.01, num_leaves=32, max_depth=8,
    min_child_samples=30, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=0.1
)

ensemble = VotingClassifier(
    estimators=[
        ('catboost', cat),
        ('sgd', sgd),
        ('lgbm', lgbm),
        ('randomforest', rf),
        ('adaboost',ada),
        ('bagging', bag),
        ('extratree',et),
        ('xgb', xgb)
    ],
    voting='soft'  # Use 'hard' for majority vote
)

ensemble.fit(X_train, y_train)
y_pred = ensemble.predict(X_test)

print(classification_report(y_test, y_pred))

print(accuracy_score(y_test, y_pred))

print(precision_score(y_test, y_pred))

print(recall_score(y_test, y_pred))

print(f1_score(y_test, y_pred))