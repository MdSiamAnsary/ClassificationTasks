# -*- coding: utf-8 -*-
"""Code[bert-base-uncased].ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EE_grsWlVy5rQz5DoqVIcnS5LpZwMClV
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install catboost

!pip install datasets

!pip install lightgbm

import sys
import gc

import pandas as pd
from sklearn.model_selection import StratifiedKFold
import numpy as np
from sklearn.metrics import roc_auc_score
import numpy as np
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.feature_extraction.text import TfidfVectorizer

from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    processors,
    trainers,
    Tokenizer,
)

from datasets import Dataset
from tqdm.auto import tqdm
from transformers import PreTrainedTokenizerFast

from sklearn.linear_model import SGDClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import VotingClassifier

dataset = pd.read_csv('dataset.csv')

dataset.head()

df = dataset.drop_duplicates(subset=['text'])
df.reset_index(drop=True, inplace=True)

df.head()

df.drop(['prompt_name', 'source', 'RDizzl3_seven'], axis='columns', inplace=True)

df.head()

df.shape

import sklearn
df = sklearn.utils.shuffle(df)

df.shape

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

!pip install sentence-transformers scikit-learn catboost lightgbm

from sentence_transformers import SentenceTransformer
from sklearn.ensemble import VotingClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import SGDClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report
import numpy as np

texts = df['text'].astype(str).tolist()
labels = df['label'].tolist()

from transformers import AutoTokenizer, AutoModel
import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

from transformers import BertTokenizer, BertModel

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')
bert_model.to(device)
bert_model.eval()

def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output.last_hidden_state  # (batch_size, seq_len, hidden_size)
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    return (token_embeddings * input_mask_expanded).sum(1) / input_mask_expanded.sum(1)

class TextDataset(Dataset):
    def __init__(self, texts):
        self.texts = texts

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        return self.texts[idx]

def encode_texts(texts, batch_size=16):
    embeddings = []
    dataset = TextDataset(texts)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)

    for batch in tqdm(loader, desc="Encoding with BERT"):
        encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors='pt', max_length=512)
        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
        with torch.no_grad():
            model_output = bert_model(**encoded_input)
        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
        embeddings.append(sentence_embeddings.cpu().numpy())
    return np.vstack(embeddings)

from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

X = encode_texts(texts)

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, labels, test_size=0.3, random_state=42)

from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier, RandomForestClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import classification_report
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier

xgb = XGBClassifier(
    n_estimators=500, learning_rate=0.05, max_depth=5, subsample=0.8, colsample_bytree=0.8,
    reg_alpha=0.5, reg_lambda=1, use_label_encoder=False, eval_metric='logloss'
)

et = ExtraTreesClassifier(n_estimators=300, max_depth=10, min_samples_split=4, random_state=42)

bag = BaggingClassifier(n_estimators=100, max_samples=0.8, max_features=0.8, random_state=42)

ada = AdaBoostClassifier(n_estimators=200, learning_rate=0.5, random_state=42)

rf = RandomForestClassifier(n_estimators=300, max_depth=10, min_samples_split=4, random_state=42)

cat = CatBoostClassifier(iterations=500, depth=6, learning_rate=0.03, l2_leaf_reg=5, verbose=0)

sgd = SGDClassifier(
    loss='log_loss', penalty='elasticnet', alpha=1e-4, l1_ratio=0.15,
    learning_rate='optimal', early_stopping=True, max_iter=2000, random_state=42
)

lgbm = LGBMClassifier(
    n_estimators=1000, learning_rate=0.01, num_leaves=32, max_depth=8,
    min_child_samples=30, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.1, reg_lambda=0.1
)

ensemble = VotingClassifier(
    estimators=[
        ('catboost', cat),
        ('sgd', sgd),
        ('lgbm', lgbm),
        ('randomforest', rf),
        ('adaboost',ada),
        ('bagging', bag),
        ('extratree',et),
        ('xgb', xgb)
    ],
    voting='soft'  # Use 'hard' for majority vote
)

ensemble.fit(X_train, y_train)
y_pred = ensemble.predict(X_test)

print(classification_report(y_test, y_pred))

print(accuracy_score(y_test, y_pred))

print(precision_score(y_test, y_pred))

print(recall_score(y_test, y_pred))

print(f1_score(y_test, y_pred))